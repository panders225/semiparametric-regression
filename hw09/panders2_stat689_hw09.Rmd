---
title: "Homework09"
author: "Philip Anderson; panders2@tamu.edu"
date: "3/21/2018"
output: pdf_document
---

# Question 1
## Chapter 3 Exercise 1A

```{r, message=FALSE}
library("tidyverse")
library("AER")
data("HousePrices")
library("mgcv")
# take a look to see variable listing and format
str(HousePrices)
```

```{r}
fitGaussAM <- mgcv::gam(price ~ s(lotsize, k=27) + bedrooms + factor(bathrooms) +
                          factor(stories) + factor(driveway) + factor(recreation) 
                        + factor(fullbase) +
                          factor(gasheat) + factor(aircon) + garage + factor(prefer)
                        , data=HousePrices
                        , family=gaussian
                        )
```

# Question 2
## Chapter 3 Exercise 1B
Evaluate whether or not the residuals are consistent with the model assumptions.
```{r}
mgcv::gam.check(fitGaussAM)
```

From the above output, it appears that we have evidence of skewness in our residual plots (qq plot shows skewness, as does the residual histogram).  It also appears that we have some evidence of heteroscedasticity (our fitted vs. residual value plot shows an increase in residual variance as we increase the value of our predictor).  Additionally, the p-value from our console output suggests that we should be using a larger number of basis functions for smoothed predictor, _lotsize_. 

# Question 3
## Chapter 3 Exercise 1F

```{r}
# generate data for the hypothetical house described by problem 1F
X <- data.frame(lotsize=5000
                , bedrooms=3
                , bathrooms=2
                , stories=2
                , driveway="yes"
                , recreation="no"
                , fullbase="yes"
                , gasheat="yes"
                , aircon="no"
                , garage=2
                , prefer="no"
                )

X_predict <- predict(fitGaussAM, newdata=X, se.fit=T)
print("Home price given by: ")
paste0("Ca$ ", round(X_predict$fit, 2))
```

The predicted home price is given by $94,511.07.

## Chapter 3 Exercise 1G

```{r}
# generate the bounds for a 95% confidence interval for the mean house price
lwr <- X_predict$fit - (1.96*X_predict$se.fit)
upr <- X_predict$fit + (1.96*X_predict$se.fit)
paste0("95% Confidence Interval given by: (", round(lwr, 2), " , ", round(upr, 2), ")")
```

# Question 4
Run a cosso on the model in Exercise 1A, along with a stepwise regression.  Compare.

```{r, message=F}
library("cosso")
```

I am first going to prep the data to get it into a form that the COSSO package appears to be looking for (I suspect that it may struggle with factor encodings).

```{r}
# select out non-factor variables
X1 <- as.matrix(HousePrices[, c(2, 3, 4, 5, 11)])
# one-hot encode the factors
X2 <- mod_mat <- stats::model.matrix(~ -1 + driveway + recreation + fullbase + gasheat + 
                                 aircon + prefer
                               , data=HousePrices)
# complete design matrix
X <- data.frame(cbind(X1, X2))
# complete response vector
y <- HousePrices[, 1]
y_alt <- log(y)
```


```{r}
# attempt a cosso
cosso_mod <- cosso::cosso(x=X, y=y, family=c("Gaussian"))
```

```{r}
# follow-up on suspicion that lack of normality in response variable distribution may be causing some issues.
# conduct test of normality for response and log-transformed response.
print(shapiro.test(y))
print(shapiro.test(y_alt))

# y_alt follows normal distribution - retry COSSO model
```

```{r}
cosso_mod2 <- cosso::cosso(x=X, y=y_alt, family=c("Gaussian"))
```

```{r}
# maybe the way I've encoded the data is the issue.  Try again
cosso_mod3 <- cosso::cosso(x=HousePrices[, -1]
                           , y=HousePrices[, 1]
                           , family=c("Gaussian")
                           )

```

```{r}
cosso_mod4 <- cosso::cosso(x=HousePrices[, -1]
                           , y=log(HousePrices[, 1])
                           , family=c("Gaussian")
                           )

```

The COSSO package does not seem to be able to solve the matrices we are giving it.  Move on to the stepwise regression.

```{r, message=FALSE}
detach("package:mgcv", unload=TRUE)
library("gam")
```


```{r}

house_init <- gam::gam(price ~ s(lotsize, 27) + bedrooms + factor(bathrooms)
                         + factor(stories) + factor(driveway) + factor(recreation) 
                       + factor(fullbase) 
                          + factor(gasheat) + factor(aircon) + garage + factor(prefer)
                        , data=HousePrices
                       , family=gaussian
                       )

# note that I am only allowing the continuous terms the possibility of 
# entering the model as smooths
house_step <- gam::step.gam(house_init, scope=list(
                             "lotsize" = ~1 + lotsize + s(lotsize, 23)
                             , "bedrooms" = ~1 + bedrooms
                             , "bathrooms" = ~ 1 + bathrooms
                             , "stories" = ~ 1 + stories
                             , "driveway" =  ~ 1 + driveway
                             , "recreation" =  ~ 1 + recreation
                             , "fullbase" = ~ 1 + fullbase
                             , "gasheat" = ~ 1 + gasheat
                             , "aircon" = ~ 1 + aircon
                             , "garage" = ~ 1 + garage + s(garage, 23)
                             , "prefer" = ~ 1 + prefer
                            )
                            , trace=T
                            , direction="forward"
                              )


```


```{r}
names(house_step$model)[-1]
```

From the stepwise output, it appears that we could not have improved the model by taking away any terms - we appear to have selected the correct model from the beginning.  This is perhaps unsurprising - if we examine summary output for the initial model fit, we can see that all of the included terms are highly significant (see below).

```{r}
gam::summary.gam(house_init)
```


# Question 5
## Chapter 3 Question 4
### 4A
```{r, message=F}
library("aplore3")
data(icu)
```

```{r}
# take a look 
str(icu)
```
```{r}
head(icu)
```

### 4B
Select a logistic gam with the response variable being the indicator of a patient dying.
```{r}
# just to make sure nothing gets messed up, recode the live variable
icu$live <- ifelse(icu$sta=="Lived", 1, 0)
table(icu$live, icu$sta) # quick check
# initial model object
icu_init <- gam::gam(live ~ age + gender + race + ser
                       + can + crn + inf 
                         + cpr + sys + hra + pre  
                         + type + fra + po2 
                         + pco + bic + cre + 
                         loc
  , data=icu
  , family=binomial(link="logit")
)
# stepwise regression setup
icu_step <- gam::step.gam(icu_init, scope=
                          list("age" = ~ 1 + age + s(age, 2)
                               , "gender" = ~ 1 + gender
                               , "ser" = ~ 1 + ser
                               , "can" = ~ 1 + can
                               , "crn" = ~ 1 + crn
                               , "inf" = ~ 1 + inf
                               , "cpr" = ~ 1 + cpr
                               , "sys" = ~ 1 + sys + s(sys, 2)
                               , "hra" = ~ 1 + hra + s(hra, 2)
                               , "pre" = ~ 1 + pre
                               , "type" = ~ 1 + type
                               , "fra" = ~ 1 + fra 
                               , "po2" = ~ 1 + po2 
                               , "ph" = ~ 1 + ph
                               , "pco" = ~ 1 + pco 
                               , "bic" = ~ 1 + bic 
                               , "cre" = ~ 1 + cre 
                               , "loc" = ~ 1 + loc
                               ) 
                           )
```

The step trace was printed above, but let's take a look to see what the final model is:

```{r}
(step_vars <- names(icu_step$model)[-1])
```

### 4C - Use mgcv::gam to re-fit the model selected in part B with GCV used for selction of the smoothing paramters.
```{r, message=F}
library("mgcv")
refit_gcv <- mgcv::gam(live ~ race + s(age) + gender + can + cpr + sys + pre + type + 
                         ph + pco + loc
          , data=icu
          , family=binomial(link="logit")
          , method="GCV.Cp"
          )
```

```{r}
mgcv::summary.gam(refit_gcv)
print("-------")
print("gam.check output")
print("-------")
mgcv::gam.check(refit_gcv)
```

The model does not appear to be a great fit to the data - the residual histogram suffers from dramatic left-skewness. 

### 4D
Use the refitted GAM model to score a new record with the described characteristics. 
```{r}
new_rec <- data.frame(
    age=79
    , gender="Female"
    , race="White"
    , ser="Medical"
    , can="No"
    , crn="No"
    , inf="No"
    , cpr="No"
    , sys=228
    , hra=94
    , pre="No"
    , type="Emergency"
    , fra="No"
    , po2="<= 60"
    , ph=">= 7.25"
    , pco="> 45"
    , bic="< 18"
    , cre="> 2.0"
    , loc="Coma"
)
# get prediction on the response scale, rather than logit.
new_rec_pred <- mgcv::predict.gam(refit_gcv, newdata=new_rec, type="response")
print(new_rec_pred)
```

# Question 6
Compare the stepwise and cosso models for Exercise 4 in Chapter 3 

```{r}
# first, fit a cosso model
icu_cosso1 <- cosso::cosso(x=icu[3:21]
                            , y=icu$live
                            , family=c("Binomial")
                          )
```

```{r}
# try out an alternative design matrix.
X1 <- stats::model.matrix(~-1 + gender + race + ser + can + crn + inf + cpr + pre + type + fra + po2 + ph + pco + bic + cre + loc, data=icu)
X2 <- icu %>%
  dplyr::select(age, sys, hra)
y <- icu$live
X <- data.frame(cbind(X1, X2))
```

```{r}
icu_cosso2 <- cosso::cosso(x=X, y=y, family=c("Binomial"))
```

It does not appear that the cosso regularization implementation is appropriate for the design matrix we are using.  I have no model to compare to the stepwise selection model from Exercise 4 in Chapter 3.

# Question 7
Rerun the model from Chapter 3, Exercise 4 using mgcv's regularization implementation.

```{r}
icu_mgcv <- mgcv::gam(live ~ s(age) + gender + race + ser
                       + can + crn + inf 
                         + cpr + s(sys) + s(hra) + pre  
                         + type + fra + po2 
                         + pco + bic + cre + 
                         loc
  , data=icu
  , family=binomial(link="logit")
  , select=TRUE
)
(icu_mgcv_smry <- mgcv::summary.gam(icu_mgcv))
```

Final model using this selection method is given by the following formula:
```{r}
icu_mgcv_smry$p.coeff
```

# Question 8
## 7A
```{r, message=FALSE}
library("kernlab")
data(spam)
print(names(spam))
```

## 7B
Generate our testing and training data sets through randomization
```{r}
set.seed(1)
nTest <- 1000
indsTest <- base::sample(1:nrow(spam), nTest, replace=FALSE)
indsTrain <- base::setdiff(1:nrow(spam), indsTest)
spamTest <- spam[indsTest, ]
spamTrain <- spam[indsTrain, ]
```

## 7C
Fit a logistic GLM including all possible predictors.

```{r}
fitTrainFullGLM <- glm(type ~ .
                       , family=binomial(link="logit")
                       , data=spamTrain
                       )
print(summary(fitTrainFullGLM))
```

## 7D
Use a model selection strategy to select a subset of predictors.
We have quite a few predictors - let's use a convenience function to generate potential predictors in our model.

```{r}
spam_scope <- gam::gam.scope(frame=spamTrain, response=58)
```

```{r}
head(spamTrain[, 1:57])
head(spamTrain[, 58])
```

```{r, message=FALSE}
detach("package:mgcv", unload=TRUE)
```

```{r}
#spam_init <- gam::gam(type ~ .
#                       , family=binomial(link="logit")
#                       , data=spamTrain
#                       )
#
#spam_step <- gam::step.gam(object=spam_init
#                           , scope=spam_scope
#                           )
#
```

That was taking way too long, and we aren't being asked to include smoothed predictors at this point.  Let's fit this with the LASSO penalization
```{r}
library("glmnet")
```

```{r}
X=as.matrix(spamTrain[, 1:57])
y=ifelse(spamTrain[, 58]=="spam", 1, 0)

spam_cv <- glmnet::cv.glmnet(x=X
                              , y=y
                              , family="binomial"
                              , alpha=1
                              , nfolds=3 # reduce runtime
                              )

```

```{r}
(lambda_min <- spam_cv$lambda.min)
(mod_coefs <- coef(spam_cv, s=lambda_min))
```

It looks like we need most of the variables, with the exception of a few.  Let's select those out of our design matrix for the upcoming GLM.
```{r}
reduced_X <- data.frame(X) %>%
  dplyr::select(-c(num857, num415, capitalAve))
```

```{r}
#test_mod <- mgcv::gam(type ~ make + address
#                      , data=spamTrain
#                      , family=binomial(link="logit")
#                      , select=T
#                        )
#mgcv::summary.gam(test_mod)
```

```{r}
design_mat <- cbind(reduced_X, y)
head(design_mat)
```

Fit the logistic GLM based on this subset.
```{r}
design_mat <- cbind(reduced_X, y)
new_glm <- stats::glm(y ~ . 
                      , data=design_mat
                      , family=binomial(link="logit")
                      )
summary(new_glm)
```

## 7E
Make predictions for the test data classify as spam if prediction probability exceeds 0.5.

```{r}
spamTest_preds <- predict.glm(object=new_glm, newdata=spamTest, type="response")
# spam=1 in train, 2 in test
pre_conf <- data.frame(cbind(spam_fl=spamTest$type
                             , spam_pred=spamTest_preds))

pre_conf$pred_decision <- ifelse(pre_conf$spam_pred>0.5, 2, 1)
# table for pre-confusion matrix
conf_table <- table(pre_conf$spam_fl, pre_conf$pred_decision)
# calculate the misclassification rate
(sum(conf_table) - (conf_table[1,1] + conf_table[2,2])) / sum(conf_table)
```

## 7F
Fit the same model as in part E, but fit all terms with smoothing splines.  Apply the same classification criterion.

```{r, message=FALSE}
library("mgcv")
```

```{r}
refit_mod <- mgcv::gam(
                      type ~ 
                        s(make) +
s(address) +
s(all) +
s(num3d) +
s(our) +
s(over) 
#s(remove) +
#s(internet) +
#s(order) +
#s(mail) +
#s(receive) +
#s(will) +
#s(people) +
#s(report) +
#s(addresses) +
#s(free) +
#s(business) +
#s(email) +
#s(you) +
#s(credit) +
#s(your) +
#s(font) +
#s(num000) +
#s(money) +
#s(hp) +
#s(hpl) +
#s(george) +
#s(num650) +
#s(lab) +
#s(labs) +
#s(telnet) +
#s(data) +
#s(num85) +
#s(technology) +
#s(num1999) +
#s(parts) +
#s(pm) +
#s(direct) +
#s(cs) +
#s(meeting) +
#s(original) +
#s(project) +
#s(re) +
#s(edu) +
#s(table) +
#s(conference) +
#s(charSemicolon) +
#s(charRoundbracket) +
#s(charSquarebracket) +
#s(charExclamation) +
#s(charDollar) +
#s(charHash) +
#s(capitalLong) +
#s(capitalTotal)
 , data=spamTrain
, family=binomial
)
```

```{r}
refit_preds <- predict.gam(object=refit_mod, newdata=spamTest, type="response")
refit_pre_conf <- data.frame(cbind(spam_fl=spamTest$type
                        , spam_pred=refit_preds
                        ))
refit_pre_conf$pred_decision <- ifelse(refit_pre_conf$spam_pred > 0.5, 2, 1)
refit_conf_table <- table(refit_pre_conf$spam_fl, refit_pre_conf$pred_decision)
# capture our misclassification rate
(sum(refit_conf_table) - (refit_conf_table[1,1] + refit_conf_table[2,2])) / sum(refit_conf_table)

```

## 7G
Based on the above, it appears that the non-smoothed GLM has superior classification accuracy.
