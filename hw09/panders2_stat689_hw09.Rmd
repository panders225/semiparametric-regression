---
title: "Homework09"
author: "Philip Anderson; panders2@tamu.edu"
date: "3/19/2018"
output: pdf_document
---

# Chapter 3 Exercise 1A

```{r, message=FALSE}
library("AER")
data("HousePrices")
#library("mgcv")
str(HousePrices)
```

```{r}
fitGaussAM <- mgcv::gam(price ~ s(lotsize, k=27) + bedrooms + factor(bathrooms) +
                          factor(stories) + factor(driveway) + factor(recreation) + factor(fullbase) +
                          factor(gasheat) + factor(aircon) + garage + factor(prefer)
                        , data=HousePrices, family=gaussian
                        )
```


# Chapter 3 Exercise 1B

```{r}
mgcv::gam.check(fitGaussAM)
```

From the above console output, we can see a p-value that indicates we may not be using an adequate number of basis functions in the fitting of our smoothed term.  More generally, we have evidence of skewness in our residual plots (qq plot shows skewness, as does the residual histogram), along with some evidence of heteroscedasticity (our fitted vs. residual value plot shows an increase in residual variance as we increase the value of our predictor). 

# Chapter 3 Exercise 1F

```{r}
# generate data for the hypothetical house described by problem 1F
X <- data.frame(lotsize=5000
                , bedrooms=3
                , bathrooms=2
                , stories=2
                , driveway="yes"
                , recreation="no"
                , fullbase="yes"
                , gasheat="yes"
                , aircon="no"
                , garage=2
                , prefer="no"
                )

X_predict <- predict(fitGaussAM, newdata=X, se.fit=T)
print("Home price given by: ")
paste0("Ca$ ", round(X_predict$fit, 2))
```

# Chapter 3 Exercise 1G

```{r}
# generate the bounds for a 95% confidence interval
lwr <- X_predict$fit - (1.96*X_predict$se.fit)
upr <- X_predict$fit + (1.96*X_predict$se.fit)
paste0("95% Confidence Interval given by: (", round(lwr, 2), " , ", round(upr, 2), ")")
```

# Question 4
Run a cosso on the model in Exercise 1A, along with a stepwise regression.  Compare.

```{r, message=F}
library("cosso")
```

```{r}
# one-hot encode the factors  
mod_mat <- stats::model.matrix(~ driveway + recreation + fullbase + gasheat + 
                                 aircon + prefer
                               , data=HousePrices)
head(mod_mat)
```

```{r}
# prep the data for input into regularized GAM
X1 <- as.matrix(HousePrices[, c(2, 3, 4, 5, 11)])
X2 <- mod_mat <- stats::model.matrix(~ driveway + recreation + fullbase + gasheat + 
                                 aircon + prefer
                               , data=HousePrices)[, -1]
# complete design matrix
X <- cbind(X1, X2)
# complete response vector
y <- HousePrices[, 1]

```


```{r}
test_mod <- cosso::cosso(x=X, y=y, family="Gaussian")
```

```{r, message=FALSE}
library("gam")
```

```{r}

house_init <- gam::gam(price ~ s(lotsize, 27) + bedrooms + factor(bathrooms)
                         + factor(stories) + factor(driveway) + factor(recreation) + factor(fullbase) 
                          + factor(gasheat) + factor(aircon) + garage + factor(prefer)
                        , data=HousePrices
                       , family=gaussian
                       )

house_step <- gam::step.gam(house_init, scope=list(
                             "lotsize" = ~1 + lotsize + s(lotsize, 2)
                             , "bedrooms" = ~1 + bedrooms
                             , "bathrooms" = ~ 1 + bathrooms
                             , "stories" = ~ 1 + stories
                             , "driveway" =  ~ 1 + driveway
                             , "recreation" =  ~ 1 + recreation
                             , "fullbase" = ~ 1 + fullbase
                             , "gasheat" = ~ 1 + gasheat
                             , "aircon" = ~ 1 + aircon
                             , "garage" = ~ 1 + garage + s(garage, 2)
                             , "prefer" = ~ 1 + prefer
                            )
                            , trace=T
                            , direction="forward"
                              )


```

```{r}
names(house_step$model)[-1]
```

From the stepwise output, it appears that we could not have improved the model by taking away any terms - we appear to have selected the correct model from the beginning.  This is perhaps unsurprising - if we examine summary output for the initial model fit, we can see that all of the included terms are highly significant (see below).

```{r}
summary(house_init)
```


# Chapter 3 Question 4
## 4A
```{r, message=F}
library("aplore3")
library("gam")
data(icu)
```

```{r}
str(icu)
```

## 4B
```{r}
# just to make sure nothing gets messed up, recode the live variable
icu$live <- ifelse(icu$sta=="Lived", 1, 0)
table(icu$live, icu$sta) # quick check
# initial model object
fitInitial <- gam::gam(live ~ age + gender + race + ser
                       + can + crn + inf 
                         + cpr + sys + hra + pre  
                         + type + fra + po2 
                         + pco + bic + cre + 
                         loc
  , data=icu
  , family=binomial(link="logit")
)
# stepwise regression setup
stepFit <- gam::step.gam(fitInitial, scope=
                          list("age" = ~ 1 + age + s(age, 2)
                               , "gender" = ~ 1 + gender
                               , "ser" = ~ 1 + ser
                               , "can" = ~ 1 + can
                               , "crn" = ~ 1 + crn
                               , "inf" = ~ 1 + inf
                               , "cpr" = ~ 1 + cpr
                               , "sys" = ~ 1 + sys + s(sys, 2)
                               , "hra" = ~ 1 + hra + s(hra, 2)
                               , "pre" = ~ 1 + pre
                               , "type" = ~ 1 + type
                               , "fra" = ~ 1 + fra 
                               , "po2" = ~ 1 + po2 
                               , "ph" = ~ 1 + ph
                               , "pco" = ~ 1 + pco 
                               , "bic" = ~ 1 + bic 
                               , "cre" = ~ 1 + cre 
                               , "loc" = ~ 1 + loc
                               ) 
                           )
```

The step trace was printed above, but let's take a look to see what the final model is:

```{r}
(step_vars <- names(stepFit$model)[-1])
```

## 3C - Use mgcv::gam to re-fit the model selected in part B with GCV used for seelction of the smoothing paramters.
```{r}
refit_gcv <- mgcv::gam(live ~ race + s(age) + gender + can + cpr + sys + pre + type + 
                         ph + pco + loc
          , data=icu
          , family=binomial(link="logit")
          , method="GCV.Cp"
          )
```

```{r}
mgcv::summary.gam(refit_gcv)
print("-------")
print("-------")
print("-------")
mgcv::gam.check(refit_gcv)
```


## 3D
Use the refitted GAM model to score a new record with described characteristics
```{r}
new_rec <- data.frame(
    age=79
    , gender="Female"
    , race="White"
    , ser="Medical"
    , can="No"
    , crn="No"
    , inf="No"
    , cpr="No"
    , sys=228
    , hra=94
    , pre="No"
    , type="Emergency"
    , fra="No"
    , po2="<=60"
    , ph=">= 7.25"
    , pco="> 45"
    , bic="< 18"
    , cre="> 2.0"
    , loc="Coma"
)

new_rec_pred <- mgcv::predict.gam(refit_gcv, newdata=new_rec, type="response")
print(new_rec_pred)
```

## 6
Compare the stepwise and cosso models for Exercise 4 in Chapter 3 

```{r}
# first, we need to actually fit a cosso model to have something available for comparison
cosso_mod <- cosso::cosso(x=icu[3:21]
                            , y=icu$live
                            , family="Binomial"
                          )
```


# Question 7
## 7A
```{r, message=FALSE}
library("kernlab")
data(spam)
print(names(spam))
```

## 7B
Generate our testing and training data sets through randomization
```{r}
set.seed(1)
nTest <- 1000
indsTest <- sample(1:nrow(spam), nTest, replace=FALSE)
indsTrain <- setdiff(1:nrow(spam), indsTest)
spamTest <- spam[indsTest, ]
spamTrain <- spam[indsTrain, ]
```

## 7C
Fit a logistic GLM including all possible predictors.

```{r}
fitTrainFullGLM <- glm(type ~ .
                       , family=binomial(link="logit")
                       , data=spamTrain
                       )
print(summary(fitTrainFullGLM))
```

## 7D
```{r}
test_mod <- mgcv::gam(type ~ make + address
                      , data=spamTrain
                      , family=binomial(link="logit")
                      , select=T
                        )
mgcv::summary.gam(test_mod)
```

Fit the logistic GLM based on this subset.
```{r}
new_mod <- glm(type ~ make + address
               , data=spamTrain
               , family=binomial(link="logit")
               )

```

## 7E
Make predictions for the test data classify as spam if prediction probability exceeds 0.5.

```{r}
spamTest_preds <- predict.glm(object=new_mod, newdata=spamTest, type="response")
# spam=2
pre_conf <- data.frame(cbind(spam_fl=spamTest$type
                             , spam_pred=spamTest_preds))
pre_conf$pred_decision <- ifelse(pre_conf$spam_pred>0.5, 2, 1)

conf_table <- table(pre_conf$spam_fl, pre_conf$pred_decision)
caret::confusionMatrix(conf_table)
```

## 7F
Fit the same model as in part E, but fit all terms with smoothing splines.  Apply the same classification criterion.

```{r}
refit_mod <- mgcv::gam(type ~ s(make) + s(address)
                      , data=spamTrain
                      , family=binomial(link="logit")
                       )
```

```{r}
refit_preds <- predict.gam(object=refit_mod, newdata=spamTest, type="response")
refit_pre_conf <- data.frame(cbind(spam_fl=spamTest$type
                        , spam_pred=refit_preds
                        ))
refit_pre_conf$pred_decision <- ifelse(refit_pre_conf$spam_pred > 0.5, 2, 1)
refit_conf_table <- table(refit_pre_conf$spam_fl, refit_pre_conf$pred_decision)
caret::confusionMatrix(refit_conf_table)
```



